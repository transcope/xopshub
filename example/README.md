# 异常检测算例
这是异常检测算例的使用说明文档。

## 评测方案

1. 评测方案1

对于单个指标，根据算法输出的异常分数，搜索所有阈值找到其中最大的F1-score，并记录相应的准确率和召回率。

对于整个数据集，分别按以下方式进行汇总：

* 计算不同模型的最佳准确率和最佳召回率的平均值，分别记为 $\bar{P}$ 和 $\bar{R}$，并计算 $\bar{F1}$：

$$\bar{F1} = \frac {2\cdot\bar{P}\cdot\bar{R}} {\bar{P}+\bar{R}}$$

* 根据不同模型的最佳阈值得到其预测结果，汇总所有预测结果后，计算总体的准确率、召回率和F1，分别记为 $P^*$、$R^*$、$F1^*$。

2. 评测方案2

在具体应用中，运维人员往往并不关注每个时间点，而更倾向于在连续异常时间段中能否触发告警，因此[Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications](https://arxiv.org/pdf/1802.03903.pdf)中首先提出了一种调整策略，并后续被广泛使用。该策略的主要思想是，如果连续异常时间段中的任意一点被识别为异常，则认为这段时间内的所有时间点均被识别成功。

按照评测方案1的步骤，并根据该调整策略分别计算相应的$\bar{P}$、$\bar{R}$、$\bar{F1}$、$P^*$、$R^*$、$F1^*$。

3. 评测方案3

评测方案2的最大问题在于延迟，如果延迟过长其实就失去了实际意义。因此AIOps比赛的评测方案（[评测脚本](https://github.com/iopsai/iops/blob/master/evaluation/evaluation.py)）在此基础上增加了延迟的限制，只有延迟K（比赛取K=7）之内识别的异常时间段才调整为识别成功，超过延迟后识别的均被调整为识别失败。

同样按照评测方案1的步骤，并根据新调整策略分别计算相应的$\bar{P}$、$\bar{R}$、$\bar{F1}$、$P^*$、$R^*$、$F1^*$。

## 评测结果

### KPI数据集
* 评测方案1

|algorithm|$\bar{P}$|$\bar{R}$|$\bar{F1}$|$P^*$|$R^*$|$F1^*$|
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
|SR|0.2124|0.2463|0.2281|0.0527|0.3887|0.0928|

* 评测方案2

|algorithm|$\bar{P}$|$\bar{R}$|$\bar{F1}$|$P^*$|$R^*$|$F1^*$|
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
|SR|0.7006|0.8354|0.7621|0.7264|0.8414|0.7797|

* 评测方案3

|algorithm|$\bar{P}$|$\bar{R}$|$\bar{F1}$|$P^*$|$R^*$|$F1^*$|
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
|SR|0.6409|0.7034|0.6707|0.6081|0.6354|0.6214|

## 代码使用
* 读取数据集

* 数据预处理

* 模型评测

* 结果汇总